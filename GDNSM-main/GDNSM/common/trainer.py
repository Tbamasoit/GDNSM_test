import itertools
import torch
import torch.optim as optim
from torch.nn.utils.clip_grad import clip_grad_norm_
from time import time
from logging import getLogger
from GDNSM.utils.topk_evaluator import TopKEvaluator
from GDNSM.utils.utils import early_stopping, dict2str
import matplotlib.pyplot as plt
import torch.nn.functional as F
import numpy as np
import copy  # <--- æ–°å¢
# å¼•å…¥æºç æä¾›çš„å·¥å…·
from GDNSM.torch_ema import ExponentialMovingAverage 
from torch.utils.tensorboard.writer import SummaryWriter
import os
import time

class Trainer:
    def __init__(self, config, model, diffusion_model=None, mg=False):
        """
        GDNSM Trainerï¼Œå¸¦ diffusion ç”Ÿæˆè´Ÿæ ·æœ¬
        """
        self.config = config
        self.model = model
        self.diffusion_model = diffusion_model  # ç”Ÿæˆè´Ÿæ ·æœ¬çš„æ¨¡å‹
        self.mg = mg
        self.logger = getLogger()

        # é…ç½®å‚æ•°
        self.learner = config.get('learner', 'adam')
        self.learning_rate = config.get('learning_rate', 0.001)
        self.epochs = config.get('epochs', 100)
        self.eval_step = min(config.get('eval_step', 1), self.epochs)
        self.stopping_step = config.get('stopping_step', 10)
        self.clip_grad_norm = config.get('clip_grad_norm', None)
        self.valid_metric = config.get('valid_metric', 'NDCG@20').lower()
        self.valid_metric_bigger = config.get('valid_metric_bigger', True)
        self.test_batch_size = config.get('eval_batch_size', 128)
        self.device = config.get('device', 'cuda')
        self.weight_decay = config.get('weight_decay', 0.0)
        if isinstance(self.weight_decay, str):
            self.weight_decay = eval(self.weight_decay)

        self.req_training = config.get('req_training', True)
        self.start_epoch = 0
        self.cur_step = 0

        tmp_dd = {f'{j.lower()}@{k}': 0.0 for j, k in itertools.product(
            config.get('metrics', ['NDCG']), config.get('topk', [20])
        )}
        self.best_valid_score = -1
        self.best_valid_result = tmp_dd
        self.best_test_upon_valid = tmp_dd
        self.train_loss_dict = dict()
        # self.optimizer = self._build_optimizer()

        # lr_scheduler = getattr(config, 'learning_rate_scheduler', [0.96, 50])
        # fac = lambda epoch: lr_scheduler[0] ** (epoch / lr_scheduler[1])
        # self.lr_scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=fac)

        # === æ–°å¢ä¸¤ä¸ªä¼˜åŒ–å™¨ï¼Œåˆ†åˆ«æ›´æ–° Ï† å’Œ Î¸ ===
        theta_params = list(self.model.diffusion_MM.parameters())
        theta_param_ids = set(id(p) for p in theta_params)
        phi_params = [p for p in self.model.parameters() if id(p) not in theta_param_ids]

        self.opt_phi = optim.Adam(phi_params, lr=self.learning_rate, weight_decay=self.weight_decay)
        self.opt_theta = optim.Adam(theta_params, lr=self.learning_rate, weight_decay=self.weight_decay)

        # === å„è‡ªé…ä¸€ä¸ª scheduler ===
        lr_scheduler = config.get('learning_rate_scheduler', [0.96, 50])
        fac = lambda epoch: lr_scheduler[0] ** (epoch / lr_scheduler[1])

        self.lr_phi = optim.lr_scheduler.LambdaLR(self.opt_phi, lr_lambda=fac)
        self.lr_theta = optim.lr_scheduler.LambdaLR(self.opt_theta, lr_lambda=fac)


        self.eval_type = config.get('eval_type', 'full')
        self.evaluator = TopKEvaluator(config)

        # GDNSM å¤šç›®æ ‡è®­ç»ƒå‚æ•°
        self.alpha1 = config.get('alpha1', 1.0)
        self.alpha2 = config.get('alpha2', 1.0)
        self.beta = config.get('beta', 1)
        self.gamma = config.get('gamma',1)

        # ==========================================
        # # [æ–°å¢] UFNRec æ¨¡å—åˆå§‹åŒ–
        # # ==========================================
        # print("Initializing UFNRec Module...")

        # # 1. å…‹éš†æ•™å¸ˆæ¨¡å‹ (Teacher Model)
        # # ä¸ºä»€ä¹ˆè¦æ”¹ï¼šUFNRec éœ€è¦ä¸€ä¸ªç¨³å®šçš„ Teacher æ¥ç›‘ç£ Studentï¼Œé˜²æ­¢ Student åœ¨ç¿»è½¬æ ‡ç­¾åå­¦åã€‚
        # # Teacher ä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼Œæ‰€ä»¥ requires_grad=Falseã€‚
        # self.teacher_model = copy.deepcopy(self.model)
        # for p in self.teacher_model.parameters():
        #     p.requires_grad = False
        # self.teacher_model.eval() # æ°¸è¿œæ˜¯ eval æ¨¡å¼

        # # 2. UFNRec è¶…å‚æ•°
        # # m: è¿ç»­å¤šå°‘æ¬¡é«˜åˆ†æ‰è®¤å®šä¸º False Negative (æŒ–æ˜é˜ˆå€¼)
        # self.ufn_m = getattr(config, 'ufn_m', 2) 
        # # alpha: Consistency Loss çš„æƒé‡
        # self.ufn_alpha = getattr(config, 'ufn_alpha', 0.1)
        # # decay: EMA æ›´æ–°çš„è¡°å‡ç‡ (Teacher èµ°å¾—å¤šæ…¢)
        # self.ufn_decay = getattr(config, 'ufn_ema_decay', 0.999)

        # # 3. å«Œç–‘äººè®¡æ•°å™¨ (Suspect Buffer)
        # # ä¸ºä»€ä¹ˆè¦æ”¹ï¼šæˆ‘ä»¬éœ€è¦è®°å½•æ¯ä¸ª (User, Item) å‡ºç°äº†å¤šå°‘æ¬¡â€œåˆ†æ•°å€’æŒ‚â€ã€‚
        # # ä½¿ç”¨å­—å…¸ {(user_id, item_id): count} æ¥èŠ‚çœå†…å­˜ã€‚
        # self.fn_counter = {} 

        self.logger = getLogger()

        # ==========================================
        # [æ–°å¢] TensorBoard åˆå§‹åŒ–
        # ==========================================
        # æ ¹æ®å½“å‰æ—¶é—´ç”Ÿæˆå”¯ä¸€çš„æ—¥å¿—ç›®å½•ï¼Œé˜²æ­¢è¦†ç›–
        timestamp = time.strftime('%m%d_%H%M')
        # ç›®å½•ç»“æ„: runs/æ•°æ®é›†_æ¨¡å‹_æ—¶é—´
        log_dir = os.path.join("runs", f"{config['dataset']}_GDNSM_UFN_{timestamp}")
        self.writer = SummaryWriter(log_dir=log_dir)
        print(f"TensorBoard log directory: {log_dir}")
        # ==========================================
        #__getitem__
        # ================= [UFNRec Init] =================
        self.ufn_warmup = config.get('ufn_warmup', 5) # çƒ­èº« Epoch
        self.ufn_m = config.get('reverse', 2)        # é˜ˆå€¼ m
        self.ufn_alpha = config.get('lbd', 0.3)       # Consistency Loss æƒé‡
        self.ufn_decay = config.get('decay', 0.999)   # EMA decay

        # [GDNSM æ‰©æ•£æ¨¡å‹å‚æ•° - ä¿®å¤æ­¤å¤„]åé¢ä»self.configä¸­å¯¼å…¥
        # self.sched_S = config.get('sched_S', 30) 
        # self.sched_lambda = config.get('sched_lambda', 1)
        # self.num_M = config.get('num_M', 5)
        
        # [æ–°å¢] è¯»å– d_epoch å’Œ use_mm_diff
        self.d_epoch = config.get('d_epoch', 1)       # é»˜è®¤è®­ç»ƒ 5 æ¬¡
        self.use_mm_diff = config.get('use_mm_diff', True) # é»˜è®¤å¼€å¯

        # [ä¼˜åŒ–1] ä½¿ç”¨ EMA å¯¹è±¡ï¼Œè€Œä¸æ˜¯ Teacher Model
        # æ³¨æ„ï¼šè¿™é‡Œåªè¿½è¸ª phi (MCI Encoder) çš„å‚æ•°ï¼Œä¸è¿½è¸ª diffusion
        self.ema = ExponentialMovingAverage(self.model.parameters(), decay=self.ufn_decay)
        self.ema.to(self.device)
        
        # [ä¼˜åŒ–2] åŒå±‚ç¼“å­˜æœºåˆ¶
        # hard_items: {(uid, iid): True}  è®°å½•ä¸Šä¸€è½®çš„å«Œç–‘äºº
        # cnt_items: {(uid, iid): count}  è®°å½•ç´¯è®¡æ¬¡æ•°
        self.hard_items = {} 
        self.cnt_items = {}

        # ==========================================


    # # ==========================================
    # # [æ–°å¢] EMA æ›´æ–°å‡½æ•°
    # # ==========================================
    # def _update_teacher(self):
    #     """
    #     ç”¨ Student (self.model) çš„å‚æ•°æ›´æ–° Teacher (self.teacher_model)
    #     å…¬å¼: theta_teacher = decay * theta_teacher + (1 - decay) * theta_student
    #     """
    #     with torch.no_grad():
    #         for param_t, param_s in zip(self.teacher_model.parameters(), self.model.parameters()):
    #             param_t.data.mul_(self.ufn_decay)
    #             param_t.data.add_((1 - self.ufn_decay) * param_s.data)
    # # ==========================================

    def _build_optimizer(self):
        opt_dict = {
            'adam': optim.Adam,
            'sgd': optim.SGD,
            'adagrad': optim.Adagrad,
            'rmsprop': optim.RMSprop
        }
        OptimCls = opt_dict.get(self.learner.lower(), optim.Adam)
        if OptimCls == optim.Adam and self.learner.lower() not in opt_dict:
            self.logger.warning('Unrecognized optimizer, using default Adam')
        return OptimCls(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)


    @torch.no_grad()
    def evaluate(self, eval_data, is_test=False):
        self.model.eval()
        batch_matrix_list = []
        for batch in eval_data:
            scores = self.model.full_sort_predict(batch)
            masked_items = batch[1]
            scores[masked_items[0], masked_items[1]] = -1e10
            _, topk_index = torch.topk(scores, max(self.config.get('topk', [20])), dim=-1)
            batch_matrix_list.append(topk_index)
        return self.evaluator.evaluate(batch_matrix_list, eval_data, is_test=is_test)

    # def _train_epoch_bprcl_diffu(self, train_data):
    #     self.model.train()
    #     total_loss = 0.0

    #     # é…ç½®å‚æ•°
    #     d_epoch = getattr(self.config, 'd_epoch', 1)
    #     use_mm_diff = getattr(self.config, 'use_mm_diff', True)
    #     base_cl_weight = getattr(self.config, 'cl_weight', 0.01)
    #     max_cl_weight = getattr(self.config, 'max_cl_weight', 0.1)
    #     beta = getattr(self.config, 'beta', 1.0)
    #     epoch = getattr(self, 'current_epoch', 0)
    #     E = max(getattr(self.config, 'epochs', 100), 1)
    #     cl_weight = base_cl_weight + (max_cl_weight - base_cl_weight) * (epoch / E)

    #     for batch in train_data:
    #         users = batch[0]
    #         pos_items = batch[1]

    #         if len(batch) >= 3:
    #             neg_items = batch[2]
    #         else:
    #             num_items = self.model.n_items
    #             neg_items = torch.randint(0, num_items, pos_items.shape, device=pos_items.device)

    #         # ===== Phase A: Diffusion Î¸ =====
    #         if use_mm_diff:
    #             for p in self.model.parameters():
    #                 p.requires_grad = False
    #             for p in self.model.diffusion_MM.parameters():
    #                 p.requires_grad = True

    #             for _ in range(d_epoch):
    #                 with torch.no_grad():
    #                     ua_embeddings, ia_embeddings, _, _ = self.model.forward(self.model.norm_adj, train=True)
    #                     u_g = ua_embeddings[users]
    #                     x0 = ia_embeddings[pos_items]

    #                     t_cond = self.model.text_trs(self.model.text_feat[pos_items])
    #                     v_cond = self.model.image_trs(self.model.image_feat[pos_items])
    #                     labels = torch.cat([u_g, t_cond, v_cond], dim=1)

    #                 diff_loss = self.model.diffusion_MM(x0, labels, device=x0.device)

    #                 self.opt_theta.zero_grad()
    #                 diff_loss.backward()
    #                 self.opt_theta.step()

    #             for p in self.model.parameters():
    #                 p.requires_grad = True
    #             for p in self.model.diffusion_MM.parameters():
    #                 p.requires_grad = False

    #         # ===== Phase B: Ï† (BPR + CL + Diffusion Negative) =====
    #         ua_embeddings, ia_embeddings, side_embeds, content_embeds = self.model.forward(self.model.norm_adj, train=True)
    #         u_g = ua_embeddings[users]
    #         pos_g = ia_embeddings[pos_items]
    #         rand_neg_g = ia_embeddings[neg_items]

    #         # -------- Contrastive Learning Loss --------
    #         side_u, side_i = torch.split(side_embeds, [self.model.n_users, self.model.n_items], dim=0)
    #         cont_u, cont_i = torch.split(content_embeds, [self.model.n_users, self.model.n_items], dim=0)
    #         side_u, side_i = F.normalize(side_u, dim=1), F.normalize(side_i, dim=1)
    #         cont_u, cont_i = F.normalize(cont_u, dim=1), F.normalize(cont_i, dim=1)
    #         cl_loss = self.model.InfoNCE(side_i[pos_items], cont_i[pos_items], 0.2) + \
    #                 self.model.InfoNCE(side_u[users], cont_u[users], 0.2)

    #         # -------- Diffusionç”Ÿæˆè´Ÿæ ·æœ¬å¹¶åº”ç”¨åŠ¨æ€éš¾åº¦è°ƒåº¦ --------
    #         L_NEG = torch.tensor(0.0, device=pos_g.device)
    #         if use_mm_diff:
    #             neg_sets = []
    #             with torch.no_grad():
    #                 t_cond = self.model.text_trs(self.model.text_feat[pos_items])
    #                 v_cond = self.model.image_trs(self.model.image_feat[pos_items])
    #                 labels_gen = torch.cat([u_g, t_cond, v_cond], dim=1)

    #                 # ç”Ÿæˆä¸‰ç§éš¾åº¦çš„è´Ÿæ ·æœ¬ (Difficulty level 1-1, 1-2, 2)
    #                 for flag in (1, 2, 3):
    #                     steps = self.model.diffusion_MM.sample(pos_g.shape, labels_gen, flag=flag)
    #                     neg_sets.append(steps[-1])  # å–æœ€åä¸€æ­¥ä½œä¸ºè´Ÿæ ·æœ¬

    #             all_negs = torch.stack(neg_sets, dim=1)  # shape: [batch, difficulty, dim]
    #             u_norm = F.normalize(u_g, dim=1).unsqueeze(1)
    #             n_norm = F.normalize(all_negs, dim=2)
    #             cos = (u_norm * n_norm).sum(dim=2)
    #             _, order = torch.sort(cos, dim=1, descending=False)  # ä»æœ€ç›¸ä¼¼(å›°éš¾)åˆ°æœ€ä¸ç›¸ä¼¼(ç®€å•)

    #             # åŠ¨æ€éš¾åº¦è°ƒåº¦ (Curriculum Learning)
    #             Mmax = 3
    #             g = max(1, min(Mmax, int(round(Mmax * (epoch+1) / E))))  # éš epoch å¢åŠ é€‰æ‹©æ›´å¤šéš¾æ ·æœ¬
    #             for k in range(g):
    #                 idx = order[:, k]
    #                 pick = all_negs[torch.arange(all_negs.size(0)), idx]
    #                 mf, _, _ = self.model.bpr_loss(u_g, pos_g, pick)
    #                 L_NEG += mf
    #             L_NEG /= g

    #         # -------- åŸå§‹ BPR loss --------
    #         bpr_mf, bpr_emb, bpr_reg = self.model.bpr_loss(u_g, pos_g, rand_neg_g)

    #         # -------- æ€» loss --------
    #         total = bpr_mf + bpr_emb + bpr_reg + cl_weight * cl_loss
    #         if use_mm_diff:
    #             total += beta * L_NEG

    #         self.opt_phi.zero_grad()
    #         total.backward()
    #         self.opt_phi.step()

    #         total_loss += total.item()

    #     self.current_epoch = epoch + 1
    #     return total_loss / len(train_data)
    def _train_epoch_phi(self, train_data, epoch):
        self.model.train()
        total_loss = 0.0
        for batch in train_data:
            users, pos_items = batch[0], batch[1]
            neg_items = torch.randint(0, self.model.n_items, pos_items.shape, device=pos_items.device)

            ua_embeddings, ia_embeddings, side_embeds, content_embeds = self.model.forward(self.model.norm_adj, train=True)
            u_g, pos_g, neg_g = ua_embeddings[users], ia_embeddings[pos_items], ia_embeddings[neg_items]

            # BPR
            bpr_mf, bpr_emb, bpr_reg = self.model.bpr_loss(u_g, pos_g, neg_g)

            # CL
            side_u, side_i = torch.split(side_embeds, [self.model.n_users, self.model.n_items], dim=0)
            cont_u, cont_i = torch.split(content_embeds, [self.model.n_users, self.model.n_items], dim=0)
            cl_loss = self.model.InfoNCE(side_i[pos_items], cont_i[pos_items], 0.2) + \
                    self.model.InfoNCE(side_u[users], cont_u[users], 0.2)

            total = bpr_mf + bpr_emb + bpr_reg +  0.01 *cl_loss

            self.opt_phi.zero_grad()
            total.backward()
            self.opt_phi.step()
            total_loss += total.item()

        return total_loss / len(train_data)

    def _train_epoch_theta(self, train_data, epoch):
        # 1. å¦‚æœé…ç½®å…³é—­äº†æ‰©æ•£æ¨¡å‹ï¼Œç›´æ¥è¿”å› 0 Loss
        if not self.use_mm_diff:
            return 0.0
        
        self.model.train()
        total_loss = 0.0

        # Ï† (Encoder) å†»ç»“ï¼Œåªè®­ç»ƒ Î¸ (Diffusion)
        for p in self.model.parameters():
            p.requires_grad = False
        for p in self.model.diffusion_MM.parameters():
            p.requires_grad = True

        for batch in train_data:
            users, pos_items = batch[0], batch[1]

            # é¢„å…ˆè®¡ç®—æ¡ä»¶ Embedding (å› ä¸º Encoder è¢«å†»ç»“äº†ï¼Œè¿™éƒ¨åˆ†å¯¹äº d_epoch å¾ªç¯æ˜¯å›ºå®šçš„)
            # æ”¾åœ¨å¾ªç¯å¤–å¯ä»¥ç¨å¾®èŠ‚çœä¸€ç‚¹ç‚¹è®¡ç®—é‡
            with torch.no_grad():
                ua_embeddings, ia_embeddings, _, _ = self.model.forward(self.model.norm_adj, train=True)
                u_g = ua_embeddings[users]
                x0 = ia_embeddings[pos_items]

                # è·å–å¤šæ¨¡æ€ç‰¹å¾ä½œä¸ºæ¡ä»¶
                t_cond = self.model.text_trs(self.model.text_feat[pos_items])
                v_cond = self.model.image_trs(self.model.image_feat[pos_items])
                labels = torch.cat([u_g, t_cond, v_cond], dim=1)

            # [å…³é”®ä¿®å¤] å¯ç”¨ d_epoch å¾ªç¯
            # å¯¹å½“å‰ Batch åå¤è®­ç»ƒ d_epoch æ¬¡
            for _ in range(self.d_epoch):
                # æ³¨æ„ï¼šè™½ç„¶ x0 å’Œ labels æ˜¯ä¸€æ ·çš„ï¼Œä½† diffusion_MM å†…éƒ¨ä¼šéšæœºé‡‡æ ·ä¸åŒçš„ timestep t å’Œå™ªå£°
                # æ‰€ä»¥è¿™å¤šæ¬¡è®­ç»ƒæ˜¯æœ‰æ„ä¹‰çš„
                diff_loss = self.model.diffusion_MM(x0, labels, device=x0.device)
                
                self.opt_theta.zero_grad()
                diff_loss.backward()
                self.opt_theta.step()
                
                total_loss += diff_loss.item()

        # è§£å†»å›æ¥
        for p in self.model.parameters():
            p.requires_grad = True
        for p in self.model.diffusion_MM.parameters():
            p.requires_grad = False

        # è®¡ç®—å¹³å‡ Loss (æ€» Loss / (Batchæ•° * d_epoch))
        return total_loss / (len(train_data) * self.d_epoch)

    def _train_epoch_joint(self, train_data, epoch):
        self.model.train()
        # total_loss = 0.0

        # [ä¿®æ”¹] å®šä¹‰åˆ†é¡¹ Loss ç´¯åŠ å™¨
        epoch_loss_total = 0.0
        epoch_loss_bpr = 0.0
        epoch_loss_cl = 0.0
        epoch_loss_lneg = 0.0
        epoch_loss_ufn = 0.0
        epoch_loss_ufn_bce = 0.0 # ç›‘æ§å¹³ååŠ›åº¦
        epoch_loss_ufn_con = 0.0 # ç›‘æ§æ•™å¸ˆçº¦æŸ

        # ç”¨äºä¿å­˜ç›¸ä¼¼åº¦
        cos_sim_pos_list = []
        cos_sim_vneg_list = []
        cos_sim_tneg_list = []
        cos_sim_tvneg_list = []

        # åŠ¨æ€éš¾åº¦è°ƒåº¦å‚æ•°
        S = self.config.get('smoothing_S', 10)        # pacing å‚æ•° S
        lam = self.config.get('lambda_ds', 1)  # pacing å‚æ•° Î»
        M = self.config.get('num_generated_neg', 3)           # åŸºç¡€è´Ÿæ ·æœ¬æ•° (æ€»å…± 3M)

        def g(ep):
            if ep < S:
                return 0
            else:
                return int(min(3 * M, ((ep / S) ** lam) * M))

        
        # åˆ¤æ–­æ˜¯å¦å¼€å¯ UFNRec (çƒ­èº«ç»“æŸ)
        ufn_flag = epoch >= self.ufn_warmup

        for batch in train_data:
            users, pos_items = batch[0], batch[1]
            
            # [å…³é”®] å¿…é¡»åœ¨è¿™é‡Œç”Ÿæˆéšæœºè´Ÿæ ·æœ¬ IDï¼Œå› ä¸º UFNRec éœ€è¦ ID æ¥æŸ¥è¡¨
            neg_items = torch.randint(0, self.model.n_items, (pos_items.shape[0],), device=pos_items.device)
            
            # ================= [Step 1: åˆ¤å®šåè½¬åå•] =================
            # åœ¨ Forward ä¹‹å‰ï¼Œå…ˆæ£€æŸ¥å“ªäº›éšæœºè´Ÿæ ·æœ¬æ˜¯"è€ç†Ÿäºº"(FN)
            reverse_indices = [] # è®°å½• batch å†…çš„ä¸‹æ ‡ [0, 5, 10...]
            
            if ufn_flag:
                # éå†å½“å‰ batch çš„éšæœºè´Ÿæ ·æœ¬
                # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦åœ¨ CPU ä¸Šåšå­—å…¸æŸ¥è¯¢ï¼Œç¨å¾®æœ‰ç‚¹æ…¢ï¼Œä½†å¿…é¡»è¿™ä¹ˆåš
                cpu_users = users.cpu().numpy()
                cpu_negs = neg_items.cpu().numpy()
                
                for idx, (uid, iid) in enumerate(zip(cpu_users, cpu_negs)):
                    key = (uid, iid)
                    # åªæœ‰å½“å®ƒåœ¨ä¸Šä¸€è½®è¢«æ ‡è®°ä¸º Hardï¼Œä¸”ç´¯è®¡æ¬¡æ•°å¤Ÿå¤šæ—¶
                    if key in self.hard_items:
                        # å¢åŠ è®¡æ•° (æºç é€»è¾‘ï¼šæ¯æ¬¡é‡åˆ°éƒ½åŠ )
                        self.cnt_items[key] = self.cnt_items.get(key, 0) + 1
                        
                        if self.cnt_items[key] >= self.ufn_m:
                            reverse_indices.append(idx)
                
                # [æ·»åŠ è¿™è¡Œ Debug]
                if len(reverse_indices) > 0:
                    print(f"Epoch {epoch}: ç»ˆäºæŒ–åˆ°äº† {len(reverse_indices)} ä¸ªå®è—ï¼")
                else:
                    # ä½ å¤§æ¦‚ç‡ä¼šçœ‹åˆ°æ»¡å±çš„è¿™ä¸ª
                    # print("Epoch {epoch}: ç©ºç©ºå¦‚ä¹Ÿ...") 
                    pass
            # ================= [Step 2: Forward] =================            
            
            
            # 1. æ­£å‘ä¼ æ’­ (Student)
            # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦æ‹¿åˆ° neg_g (çœŸå®éšæœºè´Ÿæ ·æœ¬å‘é‡)
            ua_embeddings, ia_embeddings, side_embeds, content_embeds = self.model.forward(self.model.norm_adj, train=True)
            u_g = ua_embeddings[users]
            pos_g = ia_embeddings[pos_items]
            neg_g = ia_embeddings[neg_items] # <--- è¿™æ˜¯æˆ‘ä»¬è¦æŒ–æ˜çš„å¯¹è±¡
            
            # ================= [Step 3: è®¡ç®— UFN Loss] =================
            # ==========================================
            # [æ’å…¥] UFNRec: æŒ–æ˜ä¸å¹³åé€»è¾‘
            # ==========================================
            loss_bce = torch.tensor(0.0, device=self.device)
            loss_con = torch.tensor(0.0, device=self.device)
            loss_ufn = torch.tensor(0.0, device=self.device)

            if ufn_flag and len(reverse_indices) > 0:
                rev_idx_tensor = torch.tensor(reverse_indices, device=self.device)
                
                # 1. æ‹¿åˆ° FN æ ·æœ¬
                u_fn = u_g[rev_idx_tensor]
                i_fn = neg_g[rev_idx_tensor]
                
                # 2. BCE Loss (Label Reversing)
                logits = (u_fn * i_fn).sum(dim=1)
                loss_bce = F.binary_cross_entropy_with_logits(logits, torch.ones_like(logits))

                # 3. Consistency Loss (with EMA)
                # [ä¼˜åŒ–1] ä½¿ç”¨ context manager åˆ‡æ¢æƒé‡
                with self.ema.average_parameters():
                    # æ­¤æ—¶ self.model çš„æƒé‡å˜æˆäº† Teacher çš„æƒé‡
                    # æˆ‘ä»¬åªéœ€è¦é‡æ–°è®¡ç®—è¿™å‡ ä¸ªæ ·æœ¬çš„ embedding
                    # æ³¨æ„ï¼šä¸ºäº†çœæ˜¾å­˜ï¼Œè¿™é‡Œæœ€å¥½åªç®—å¿…è¦çš„ï¼Œä½† LightGCN ç‰¹æ€§å†³å®šäº†å¿…é¡»å…¨é‡ forward
                    # å¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œè¿™é‡Œå¯ä»¥ wrap åœ¨ torch.no_grad() é‡Œ
                    t_ua, t_ia, _, _ = self.model.forward(self.model.norm_adj, train=True)
                    t_u_fn = t_ua[users[rev_idx_tensor]]
                    t_i_fn = t_ia[neg_items[rev_idx_tensor]]
                    
                    teacher_logits = (t_u_fn * t_i_fn).sum(dim=1)
                    teacher_probs = torch.sigmoid(teacher_logits)
                
                # åˆ‡æ¢å›æ¥åï¼Œself.model å˜å› Student
                loss_con = F.binary_cross_entropy_with_logits(logits, teacher_probs)
                
                loss_ufn = loss_bce + self.ufn_alpha * loss_con


            # # A. æŒ–æ˜ (Mining)
            # with torch.no_grad():
            #     # è®¡ç®—åˆ†æ•°
            #     pos_scores = (u_g * pos_g).sum(dim=1)
            #     neg_scores = (u_g * neg_g).sum(dim=1)
                
            #     # æ‰¾åˆ°â€œå€’æŒ‚â€çš„æ ·æœ¬ (è´Ÿæ ·æœ¬å¾—åˆ† > æ­£æ ·æœ¬å¾—åˆ†)
            #     # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šå¦‚æœæ¨¡å‹è§‰å¾—è¿™ä¸ªéšæœºè´Ÿæ ·æœ¬æ¯”æ­£æ ·æœ¬è¿˜å¥½ï¼Œå®ƒå¾ˆå¯èƒ½æ˜¯ False Negative
            #     suspect_mask = neg_scores > pos_scores
            #     suspect_indices = torch.nonzero(suspect_mask).squeeze() # tensor([1, 3])  # LongTensorï¼Œè¡¨ç¤ºç¬¬1ä¸ªå’Œç¬¬3ä¸ªæ ·æœ¬æ˜¯å«Œç–‘äºº

            #     # æ›´æ–°è®¡æ•°å™¨
            #     confirmed_fn_indices = [] # æœ¬æ¬¡ batch ç¡®è®¤çš„ FN ç´¢å¼•
            #     if suspect_indices.numel() > 0: # numel() æ˜¯ number of elements (å…ƒç´ ä¸ªæ•°)
            #         # å¤„ç† scalar è¿™é‡Œçš„å‘
            #         if suspect_indices.dim() == 0: suspect_indices = suspect_indices.unsqueeze(0)
                    
            #         for idx in suspect_indices:
            #             idx = idx.item()
            #             uid = users[idx].item()
            #             iid = neg_items[idx].item()
            #             key = (uid, iid)
                        
            #             # è®¡æ•° +1
            #             self.fn_counter[key] = self.fn_counter.get(key, 0) + 1
                        
            #             # åˆ¤æ–­æ˜¯å¦è¾¾åˆ°é˜ˆå€¼ m
            #             if self.fn_counter[key] >= self.ufn_m:
            #                 confirmed_fn_indices.append(idx)
            # B. è®¡ç®—å¹³å Loss (Reversing & Consistency)
            # if len(confirmed_fn_indices) > 0:
            #     # æ‹¿åˆ°è¢«ç¡®è®¤ä¸º FN çš„æ ·æœ¬å‘é‡
            #     fn_idx_tensor = torch.tensor(confirmed_fn_indices, device=self.device)
            #     u_fn = u_g[fn_idx_tensor]
            #     i_fn = neg_g[fn_idx_tensor] # è¿™äº›åŸæœ¬æ˜¯è´Ÿæ ·æœ¬ï¼Œç°åœ¨æˆ‘ä»¬è¦ç»™å®ƒå¹³å

            #     # --- Loss 1: BCE Loss (Label Reversing) ---
            #     # å¼ºè¡ŒæŠŠ Label è®¾ä¸º 1
            #     logits = (u_fn * i_fn).sum(dim=1)
            #     labels = torch.ones_like(logits)
            #     loss_ufn_bce = F.binary_cross_entropy_with_logits(logits, labels)
                
            #     # --- Loss 2: Consistency Loss (Teacher Supervision) ---
            #     # è®© Teacher ä¹Ÿæ¥çœ‹çœ‹è¿™äº›æ ·æœ¬
            #     with torch.no_grad():
            #         # Teacher ä¹Ÿè¦åšä¸€æ¬¡ forward æ‹¿åˆ°æœ€æ–°çš„ Embedding
            #         t_ua, t_ia, _, _ = self.teacher_model.forward(self.model.norm_adj, train=True)
            #         t_u_fn = t_ua[users[fn_idx_tensor]]
            #         t_i_fn = t_ia[neg_items[fn_idx_tensor]]
                    
            #         # Teacher çš„æ‰“åˆ† (Soft Label)
            #         teacher_logits = (t_u_fn * t_i_fn).sum(dim=1)
            #         teacher_probs = torch.sigmoid(teacher_logits)

            #     # Student çš„é¢„æµ‹æ¦‚ç‡
            #     # student_probs = torch.sigmoid(logits)
                
            #     # è®¡ç®—ä¸€è‡´æ€§ Loss (è¿™é‡Œç”¨ BCE å½¢å¼çš„è’¸é¦)
            #     # ç›®æ ‡æ˜¯è®© Student çš„æ¦‚ç‡å»é€¼è¿‘ Teacher çš„æ¦‚ç‡
            #     # loss_ufn_con = -(teacher_probs * torch.log(student_probs + 1e-8) + 
            #     #                  (1 - teacher_probs) * torch.log(1 - student_probs + 1e-8)).mean()
                
            #     # æ³¨æ„ï¼šè¿™é‡Œä¼ å…¥çš„æ˜¯ student_logits (æœªæ¿€æ´»)ï¼Œè€Œä¸æ˜¯ student_probs
            #     loss_ufn_con = F.binary_cross_entropy_with_logits(logits, teacher_probs)
            # # ==========================================


            # 2. åŸæœ‰çš„ Loss è®¡ç®— (ä¿æŒä¸å˜)
            bpr_mf, bpr_emb, bpr_reg = self.model.bpr_loss(u_g, pos_g, neg_g)

            # CL
            side_u, side_i = torch.split(side_embeds, [self.model.n_users, self.model.n_items], dim=0)
            cont_u, cont_i = torch.split(content_embeds, [self.model.n_users, self.model.n_items], dim=0)
            cl_loss = self.model.InfoNCE(side_i[pos_items], cont_i[pos_items], 0.2) + \
                    self.model.InfoNCE(side_u[users], cont_u[users], 0.2)

            # Î¸ ç”Ÿæˆè´Ÿæ ·æœ¬ (å›ºå®š Î¸)
            with torch.no_grad():
                t_cond = self.model.text_trs(self.model.text_feat[pos_items])
                v_cond = self.model.image_trs(self.model.image_feat[pos_items])
                labels_gen = torch.cat([u_g, t_cond, v_cond], dim=1)

                # ä¸‰ç§ç”Ÿæˆè´Ÿæ ·æœ¬
                O_v_all, O_t_all, O_tv_all = [], [], []
                for _ in range(M):
                    O_v_all.append(self.model.diffusion_MM.sample(pos_g.shape, labels_gen, flag=1)[-1])
                    O_t_all.append(self.model.diffusion_MM.sample(pos_g.shape, labels_gen, flag=2)[-1])
                    O_tv_all.append(self.model.diffusion_MM.sample(pos_g.shape, labels_gen, flag=3)[-1])


            with torch.no_grad(): 
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                u_norm = F.normalize(u_g, dim=1)             # [B, D]
                pos_norm = F.normalize(pos_g, dim=1)
                for vneg, tneg, tvneg in zip(O_v_all, O_t_all, O_tv_all):
                    vneg_norm = F.normalize(vneg, dim=1)
                    tneg_norm = F.normalize(tneg, dim=1)
                    tvneg_norm = F.normalize(tvneg, dim=1)
                    cos_sim_pos_list.extend((u_norm * pos_norm).sum(dim=1).cpu().numpy())
                    cos_sim_vneg_list.extend((u_norm * vneg_norm).sum(dim=1).cpu().numpy())
                    cos_sim_tneg_list.extend((u_norm * tneg_norm).sum(dim=1).cpu().numpy())
                    cos_sim_tvneg_list.extend((u_norm * tvneg_norm).sum(dim=1).cpu().numpy())

            # åŠ¨æ€éš¾åº¦è°ƒåº¦
            Q_diff = O_v_all + O_t_all + O_tv_all
            Q_diff = Q_diff[:g(epoch+1)]


            # è®¡ç®— L_NEG
            L_NEG = 0.0
            for hard_neg in Q_diff:
                mf_neg, _, _ = self.model.bpr_loss(u_g, pos_g, hard_neg)
                L_NEG += mf_neg
            if len(Q_diff) > 0:
                L_NEG /= len(Q_diff)
            if epoch<S:
                L_NEG = 0.0

            # æ€»æŸå¤±
            # [ä¿®æ”¹] åŠ å…¥ UFNRec çš„ loss
            total = bpr_mf + bpr_emb + bpr_reg + 0.01* cl_loss + self.beta * L_NEG + self.gamma * loss_ufn
                    
            self.opt_phi.zero_grad()
            total.backward()
            self.opt_phi.step()

            # [ä¼˜åŒ–1] æ›´æ–° EMA
            if ufn_flag:
                if self.ema.shadow_params[0].device != next(self.model.parameters()).device:
                    self.ema.to(next(self.model.parameters()).device)
                    
                self.ema.update()


            # ================= [Step 5: Mining (äº‹åæŒ–æ˜)] =================
            # [ä¼˜åŒ–2] åœ¨å‚æ•°æ›´æ–°åï¼Œåˆ©ç”¨æœ€æ–°çš„ Logits æ›´æ–° hard_items
            if ufn_flag:
                with torch.no_grad():
                    # é‡æ–°ç®—åˆ† (ç”¨æ›´æ–°åçš„å‚æ•°ï¼Œæˆ–è€…ç›´æ¥å¤ç”¨ backward å‰çš„ logits ä¹Ÿå¯ä»¥ï¼Œæºç æ˜¯ç”¨ backward å‰çš„)
                    # ä¸ºäº†æ•ˆç‡ï¼Œæˆ‘ä»¬å¤ç”¨ backward å‰çš„ u_g, pos_g, neg_g
                    # è™½ç„¶å‚æ•°æ›´æ–°äº†ä¸€ç‚¹ç‚¹ï¼Œä½†å½±å“ä¸å¤§
                    pos_scores = (u_g * pos_g).sum(dim=1)
                    neg_scores = (u_g * neg_g).sum(dim=1)
                    
                    # æ‰¾åˆ°å€’æŒ‚
                    suspect_mask = neg_scores > pos_scores
                    suspect_indices = torch.nonzero(suspect_mask).squeeze()
                    
                    if suspect_indices.numel() > 0:
                        if suspect_indices.dim() == 0: suspect_indices = suspect_indices.unsqueeze(0)
                        
                        # æ¸…ç©ºä¸Šä¸€è½®çš„ hard_items? æºç é€»è¾‘æ˜¯åŠ¨æ€ç»´æŠ¤
                        # è¿™é‡Œæˆ‘ä»¬ç®€å•ç‚¹ï¼šæ¯è½®éƒ½æŠŠæ–°å‘ç°çš„åŠ è¿›å»
                        cpu_idx = suspect_indices.cpu().numpy()
                        for idx in cpu_idx:
                            uid = users[idx].item()
                            iid = neg_items[idx].item()
                            # è®°å½•åˆ° hard_itemsï¼Œè¿™å°±æˆä¸ºäº†ä¸‹ä¸€è½®çš„å€™é€‰äºº
                            self.hard_items[(uid, iid)] = True
            
            # [ç›‘æ§] ç´¯åŠ æœ¬ Batch çš„ Loss
            epoch_loss_total += total.item()
            epoch_loss_bpr += bpr_mf.item()
            epoch_loss_cl += cl_loss.item()
            epoch_loss_lneg += L_NEG.item() if isinstance(L_NEG, torch.Tensor) else L_NEG
            epoch_loss_ufn += loss_ufn.item()
            epoch_loss_ufn_bce += loss_bce.item()
            epoch_loss_ufn_con += loss_con.item()


            # total_loss += total.item()

        # æ‰“å°ä½™å¼¦ç›¸ä¼¼åº¦åˆ†å¸ƒ
        print("Cosine similarity distributions:")
        print(f"Positive samples: mean={np.mean(cos_sim_pos_list):.4f}, std={np.std(cos_sim_pos_list):.4f}")
        print(f"Video Negatives: mean={np.mean(cos_sim_vneg_list):.4f}, std={np.std(cos_sim_vneg_list):.4f}")
        print(f"Text Negatives: mean={np.mean(cos_sim_tneg_list):.4f}, std={np.std(cos_sim_tneg_list):.4f}")
        print(f"Video+Text Negatives: mean={np.mean(cos_sim_tvneg_list):.4f}, std={np.std(cos_sim_tvneg_list):.4f}")
        
        # ==========================================
        # [æ ¸å¿ƒ] å†™å…¥ TensorBoard
        # ==========================================
        num_batches = len(train_data)
        self.writer.add_scalar('Loss/Total', epoch_loss_total / num_batches, epoch)
        self.writer.add_scalar('Loss/BPR_Basic', epoch_loss_bpr / num_batches, epoch)
        self.writer.add_scalar('Loss/CL', epoch_loss_cl / num_batches, epoch)
        self.writer.add_scalar('Loss/Diffusion_Gen', epoch_loss_lneg / num_batches, epoch)
        
        # é‡ç‚¹å…³æ³¨è¿™ä¸‰ä¸ªï¼å¦‚æœä¸ä¸º0ï¼Œè¯´æ˜ç”Ÿæ•ˆäº†
        self.writer.add_scalar('Loss/UFN_Total', epoch_loss_ufn / num_batches, epoch)
        self.writer.add_scalar('Loss/UFN_BCE', epoch_loss_ufn_bce / num_batches, epoch)
        self.writer.add_scalar('Loss/UFN_Consistency', epoch_loss_ufn_con / num_batches, epoch)
        # ==========================================

        return epoch_loss_total / len(train_data)



    # def fit(self, train_data, valid_data=None, test_data=None, saved=True, verbose=True):
    #     """
    #     ä¸‰é˜¶æ®µè®­ç»ƒ:
    #     1. æ¨èæ¨¡å‹ Ï† é¢„è®­ç»ƒ (BPR+CL)
    #     2. æ‰©æ•£æ¨¡å‹ Î¸ è®­ç»ƒ (Ï† å›ºå®š)
    #     3. è”åˆé˜¶æ®µ (Î¸ å›ºå®š, Ï† æ›´æ–°, ç”Ÿæˆè´Ÿæ ·æœ¬)
    #     """
    #     E1 = getattr(self.config, 'pretrain_epochs', 10)
    #     E2 = getattr(self.config, 'diff_epochs', 10)
    #     E3 = getattr(self.config, 'joint_epochs', 30)

    #     self.best_valid_score = -1e9 if self.valid_metric_bigger else 1e9
    #     self.best_valid_result, self.best_test_upon_valid = {}, {}

    #     print(f"\n===> å¼€å§‹ä¸‰é˜¶æ®µè®­ç»ƒï¼Œæ€»è½®æ•° {E1+E2+E3}")

    #     # -------- é˜¶æ®µ 1: æ¨èæ¨¡å‹ Ï† --------
    #     print(f"\n[é˜¶æ®µ 1] é¢„è®­ç»ƒæ¨èæ¨¡å‹ Ï† ({E1} è½®)")
    #     for epoch in range(E1):
    #         loss = self._train_epoch_phi(train_data, epoch)
    #         if verbose:
    #             self.logger.info(f"[é˜¶æ®µ 1] Epoch {epoch+1}/{E1} | Loss={loss:.4f}")
    #         # éªŒè¯
    #         if valid_data and (epoch + 1) % self.eval_step == 0:
    #             self._do_validation(epoch, valid_data, test_data, saved, verbose)

    #     # -------- é˜¶æ®µ 2: æ‰©æ•£æ¨¡å‹ Î¸ --------
    #     print(f"\n[é˜¶æ®µ 2] è®­ç»ƒæ‰©æ•£æ¨¡å‹ Î¸ (ç›´åˆ°æ”¶æ•›, Ï† å›ºå®š)")

    #     patience = getattr(self.config, 'diff_patience', 5)   # è¿ç»­å¤šå°‘æ¬¡æ²¡æå‡å°±åœ
    #     min_delta = getattr(self.config, 'diff_min_delta', 1e-2)  # æœ€å°æ”¹å–„å¹…åº¦
    #     best_loss = float('inf')
    #     bad_count = 0
    #     epoch = 0

    #     while True:
    #         loss = self._train_epoch_theta(train_data, epoch)
    #         if verbose:
    #             self.logger.info(f"[é˜¶æ®µ 2] Epoch {epoch+1} | Diff Loss={loss:.4f}")

    #         # éªŒè¯
    #         if valid_data and (epoch + 1) % self.eval_step == 0:
    #             self._do_validation(epoch, valid_data, test_data, saved, verbose)

    #         # æ”¶æ•›åˆ¤æ–­
    #         if best_loss - loss > min_delta:
    #             best_loss = loss
    #             bad_count = 0
    #         else:
    #             bad_count += 1

    #         if bad_count >= patience:
    #             print(f"Î¸ è®­ç»ƒåœ¨ {epoch+1} è½®æ—¶æå‰æ”¶æ•› âœ… (æœ€ä¼˜ Loss={best_loss:.4f})")
    #             break

    #         epoch += 1


    #     # -------- é˜¶æ®µ 3: è”åˆé˜¶æ®µ --------
    #     print(f"\n[é˜¶æ®µ 3] è”åˆè®­ç»ƒ Ï†+Î¸ ({E3} è½®, Î¸ å›ºå®š)")
    #     for epoch in range(E3):
    #         loss = self._train_epoch_joint(train_data, epoch)
    #         if verbose:
    #             self.logger.info(f"[é˜¶æ®µ 3] Epoch {epoch+1}/{E3} | Joint Loss={loss:.4f}")
    #         if valid_data and (epoch + 1) % self.eval_step == 0:
    #             self._do_validation(epoch, valid_data, test_data, saved, verbose)

    #     print("\n===> ä¸‰é˜¶æ®µè®­ç»ƒå®Œæˆ âœ…")
    #     return self.best_valid_score, self.best_valid_result, self.best_test_upon_valid

    def _do_validation(self, epoch, valid_data, test_data, saved=True, verbose=True):
        valid_result = self.evaluate(valid_data)
        valid_score = valid_result.get(self.valid_metric, valid_result.get('NDCG@20', 0.0))
        test_result = self.evaluate(test_data, is_test=True) if test_data else None

        # ==========================================
        # [æ–°å¢] å†™å…¥éªŒè¯é›†æŒ‡æ ‡
        # ==========================================
        # valid_result é€šå¸¸æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« 'Recall@10', 'NDCG@20' ç­‰
        for metric, value in valid_result.items():
            self.writer.add_scalar(f'Metric/Valid/{metric}', value, epoch)
        
        # å¦‚æœæœ‰æµ‹è¯•é›†ç»“æœï¼Œä¹Ÿå†™è¿›å»
        if test_data and test_result:
            for metric, value in test_result.items():
                self.writer.add_scalar(f'Metric/Test/{metric}', value, epoch)
        # ==========================================
        

        # åˆ¤æ–­æ˜¯å¦æ›´æ–° best
        is_better = (valid_score > self.best_valid_score) if self.valid_metric_bigger else (valid_score < self.best_valid_score)
        if is_better:
            self.best_valid_score = valid_score
            self.best_valid_result = valid_result
            self.best_test_upon_valid = test_result
            if saved:
                save_path = f'best_model_{self.config.get("model", "GDNSM")}.pth'
                torch.save(self.model.state_dict(), save_path)
                self.logger.info(f'Best model saved to {save_path}')

        if verbose:
            self.logger.info(f"Epoch {epoch} | Valid: {valid_score:.4f}")
            if test_result:
                self.logger.info(f"Test Result: {dict2str(test_result)}")
        # [ä¿®æ”¹] è¿”å›æ˜¯å¦å˜å¾—æ›´å¥½ (True/False)
        return is_better
    
    def fit(self, train_data, valid_data=None, test_data=None, saved=True, verbose=True):
        """
        æ¯ä¸ª epoch éƒ½æ‰§è¡Œä¸‰ä¸ªé˜¶æ®µ:
        1. æ›´æ–°æ¨èæ¨¡å‹ Ï† (BPR+CL)
        2. æ›´æ–°æ‰©æ•£æ¨¡å‹ Î¸ (Ï† å†»ç»“)
        3. è”åˆè®­ç»ƒ (Î¸ å›ºå®š, Ï† æ›´æ–°, ç”Ÿæˆè´Ÿæ ·æœ¬)
        """
        E = self.config.get('total_epochs', 100)  # æ€» epoch æ•°
        self.best_valid_score = -1e9 if self.valid_metric_bigger else 1e9
        self.best_valid_result, self.best_test_upon_valid = {}, {}

        # [æ—©åœå‚æ•°]
        patience = self.stopping_step  # ä» config è¯»å–ï¼Œé»˜è®¤é€šå¸¸æ˜¯ 10 æˆ– 20
        wait = 0                       # å¿è€è®¡æ•°å™¨

        print(f"\n===> å¼€å§‹è”åˆè®­ç»ƒ (æ¯ä¸ª epoch åŒ…å« 3 ä¸ªé˜¶æ®µ)ï¼Œæ€»è½®æ•° {E}")

        for epoch in range(E):
            # -------- é˜¶æ®µ 1: æ¨èæ¨¡å‹ Ï† --------
            loss_phi = self._train_epoch_phi(train_data, epoch)
            
            # -------- é˜¶æ®µ 2: æ‰©æ•£æ¨¡å‹ Î¸ --------
            loss_theta = self._train_epoch_theta(train_data, epoch)

            # -------- é˜¶æ®µ 3: è”åˆè®­ç»ƒ --------
            loss_joint = self._train_epoch_joint(train_data, epoch)

            if verbose:
                self.logger.info(
                    f"Epoch {epoch+1}/{E} | "
                    f"Phi Loss={loss_phi:.4f} | "
                    f"Theta Loss={loss_theta:.4f} | "
                    f"Joint Loss={loss_joint:.4f}"
                )

            # éªŒè¯
            if valid_data and (epoch + 1) % self.eval_step == 0:
                # æ¥æ”¶è¿”å›å€¼ï¼šæ˜¯å¦å˜å¥½äº†ï¼Ÿ
                is_better = self._do_validation(epoch, valid_data, test_data, saved, verbose)
                
                if is_better:
                    wait = 0  # ç ´çºªå½•äº†ï¼Œé‡ç½®å¿è€å€¼
                else:
                    wait += 1 # æ²¡ç ´çºªå½•ï¼Œå¿è€å€¼ +1
                    if verbose:
                        print(f"   [EarlyStopping] Patience: {wait}/{patience}")

                # 3. è§¦å‘æ—©åœ
                if wait >= patience:
                    print(f"\nğŸ›‘ è§¦å‘æ—©åœ (Early Stopping)ï¼åœ¨è¿ç»­ {patience} ä¸ª Epoch å†…éªŒè¯é›†æŒ‡æ ‡æœªæå‡ã€‚")
                    print(f"æœ€ä½³éªŒè¯é›†å¾—åˆ†: {self.best_valid_score:.4f}")
                    break


        print("\n===> è®­ç»ƒå®Œæˆ âœ…")
        return self.best_valid_score, self.best_valid_result, self.best_test_upon_valid





    def plot_train_loss(self, save_path=None):
        epochs = sorted(self.train_loss_dict.keys())
        losses = [self.train_loss_dict[e] for e in epochs]
        plt.plot(epochs, losses)
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training Loss')
        plt.xticks(epochs)
        plt.grid(True)
        if save_path:
            plt.savefig(save_path)
        plt.show()
